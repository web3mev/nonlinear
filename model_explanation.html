
<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>Model Explanation</title>
    <style>
        body { font-family: sans-serif; line-height: 1.6; max-width: 800px; margin: 0 auto; padding: 20px; }
        h1, h2, h3 { color: #333; }
        code { background: #f4f4f4; padding: 2px 5px; border-radius: 3px; }
        pre { background: #f4f4f4; padding: 10px; border-radius: 5px; overflow-x: auto; }
        /* Ensure math is visible */
        .MathJax { font-size: 110%; }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<h1>Model Formula and Monotonicity Constraints</h1>
<p>This document explains the mathematical formulation of the nonlinear fitting model and how monotonicity constraints are enforced for 1D and 2D parameters.</p>
<h2>1. Model Overview</h2>
<p>The model predicts a response variable $Y$ based on a set of risk factors (parameters). The relationship is modeled as:</p>
<p>$$ Y = \exp\left( \sum\_{k} f_k(X_k) \right) \cdot \epsilon $$</p>
<p>Taking the logarithm, we get a linear additive model:</p>
<p>$$ \ln(Y) = \sum\_{k} f_k(X_k) + \ln(\epsilon) $$</p>
<p>where $f_k(X_k)$ is the contribution of the $k$-th risk factor. The function $f_k$ can be:</p>
<ul>
<li><strong>DIM_0</strong>: A linear term $f(x) = \beta \cdot x$.</li>
<li><strong>DIM_1</strong>: A piecewise linear function (spline) of one variable.</li>
<li><strong>DIM_2</strong>: A piecewise bilinear surface of two variables.</li>
</ul>
<h2>2. One-Dimensional Monotonicity (DIM_1)</h2>
<p>For a 1D parameter with knots $k_0, k_1, \dots, k_n$, the function values at the knots are $v_0, v_1, \dots, v_n$. The function is interpolated linearly between knots.</p>
<h3>Monotonic Increasing</h3>
<p>To enforce that the function is monotonically increasing ($v_{i+1} \ge v_i$), we parameterize the values using non-negative increments (deltas):</p>
<p>$$ v*0 = P_0 $$
$$ v_i = v*{i-1} + \delta_i \quad \text{for } i=1 \dots n $$</p>
<p>where $\delta_i \ge 0$.</p>
<p>In the optimization, we solve for $P_0$ (unbounded) and $\delta_1, \dots, \delta_n$ (lower bound 0).</p>
<h3>Monotonic Decreasing</h3>
<p>Similarly, for monotonically decreasing functions:</p>
<p>$$ v*i = v*{i-1} - \delta_i \quad \text{for } i=1 \dots n $$</p>
<p>where $\delta_i \ge 0$.</p>
<h2>3. Two-Dimensional Monotonicity (DIM_2)</h2>
<p>For a 2D parameter with knots $u_0, \dots, u_R$ (rows) and $v_0, \dots, v_C$ (columns), we define a grid of values $Z_{i,j}$.</p>
<p>We want to enforce monotonicity in both dimensions. For example, "Increasing/Increasing" means:</p>
<ul>
<li>$Z_{i+1, j} \ge Z_{i, j}$ (Increasing in U)</li>
<li>$Z_{i, j+1} \ge Z_{i, j}$ (Increasing in V)</li>
</ul>
<p>This is achieved using a parameterization similar to <strong>Isotonic Regression</strong> on a grid.</p>
<h3>Parameterization</h3>
<p>We define the grid values $Z_{i,j}$ using a set of non-negative parameters:</p>
<ul>
<li>$z_{00}$: Base value at origin (unbounded).</li>
<li>$d^u_i$: Increments along the first column ($i=0 \dots R-2$).</li>
<li>$d^v_j$: Increments along the first row ($j=0 \dots C-2$).</li>
<li>$d^{int}_k$: Interaction increments for internal points.</li>
</ul>
<p>The reconstruction logic is:</p>
<ol>
<li>
<p><strong>Origin</strong>:
    $$ Z*{0,0} = z*{00} $$</p>
</li>
<li>
<p><strong>First Column (U-edge)</strong>:
    $$ Z*{i+1, 0} = Z*{i, 0} + d^u_i, \quad d^u_i \ge 0 $$</p>
</li>
<li>
<p><strong>First Row (V-edge)</strong>:
    $$ Z*{0, j+1} = Z*{0, j} + d^v_j, \quad d^v_j \ge 0 $$</p>
</li>
<li>
<p><strong>Internal Points</strong>:
    For $i > 0, j > 0$, the value $Z_{i,j}$ must be greater than or equal to both its left neighbor $Z_{i, j-1}$ and its upper neighbor $Z_{i-1, j}$.</p>
<p>$$ Z*{i, j} = \max(Z*{i-1, j}, Z*{i, j-1}) + d^{int}*{k}, \quad d^{int}\_{k} \ge 0 $$</p>
</li>
</ol>
<p>This construction guarantees that every point is greater than or equal to its predecessors in both directions, ensuring global monotonicity on the grid.</p>
<h3>Other Directions</h3>
<p>For other monotonicity combinations (e.g., Increasing/Decreasing), the grid is flipped internally before applying the constraints, and then flipped back.</p>
<ul>
<li><strong>Inc/Dec</strong>: Flip V-axis, apply Inc/Inc, flip back.</li>
<li><strong>Dec/Inc</strong>: Flip U-axis, apply Inc/Inc, flip back.</li>
<li><strong>Dec/Dec</strong>: Flip both axes, apply Inc/Inc, flip back.</li>
</ul>
<h2>4. Performance Metrics</h2>
<p>The following metrics are used to evaluate model performance:</p>
<h3>R-squared ($R^2$)</h3>
<p>The proportion of variance in the dependent variable explained by the model:</p>
<p>$$ R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2} $$</p>

<h3>Pseudo R-squared (McFadden)</h3>
<p>A measure of fit for non-linear models comparable to $R^2$. It compares the likelihood of the fitted model to a null model (intercept only).</p>
<p>$$ R^2_{pseudo} = 1 - \frac{\ln(L_{model})}{\ln(L_{null})} \approx 1 - \frac{\chi^2_{model}}{\chi^2_{null}} $$</p>

<h3>RMSE (Root Mean Square Error)</h3>
<p>Standard deviation of the residuals:</p>
<p>$$ RMSE = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2} $$</p>

<h3>MAE (Mean Absolute Error)</h3>
<p>Average magnitude of the errors:</p>
<p>$$ MAE = \frac{1}{N} \sum_{i=1}^{N} |y_i - \hat{y}_i| $$</p>

<h3>Gini Coefficient</h3>
<p>A measure of the model's ability to rank risk. Derived from the Area Under the Lorenz Curve (AUC).</p>
<p>$$ Gini = 2 \cdot AUC - 1 $$</p>

<h3>Poisson Deviance</h3>
<p>Goodness-of-fit statistic for Poisson regression:</p>
<p>$$ D = 2 \sum_{i=1}^{N} \left[ y_i \ln\left(\frac{y_i}{\hat{y}_i}\right) - (y_i - \hat{y}_i) \right] $$</p>
<h2>5. Performance Chart Logic (Distributed Weighting)</h2>
<p>The "Weighted Actual" and "Weighted Model" charts are calculated by distributing every data point's weight to its adjacent knots, ensuring a robust representation even with sparse data.</p>

<h3>1D Distribution Logic</h3>
<p>For a data point $(x, y)$ with weight $w$, falling between knots $k_{i}$ and $k_{i+1}$:</p>
<p>
    $$ \alpha = \frac{x - k_i}{k_{i+1} - k_i} $$
</p>
<p>The point contributes to the weighted sum at both knots:</p>
<ul>
    <li>At $k_i$: contributes $(1-\alpha) \cdot w \cdot y$ to the numerator and $(1-\alpha) \cdot w$ to the denominator.</li>
    <li>At $k_{i+1}$: contributes $\alpha \cdot w \cdot y$ to the numerator and $\alpha \cdot w$ to the denominator.</li>
</ul>
<p>The final plotted value at knot $k$ is:</p>
<p>
    $$ \text{Value}_k = \frac{\sum \text{Distributed Numerator}}{\sum \text{Distributed Denominator}} $$
</p>

<h3>2D Distribution Logic (Bilinear)</h3>
<p>Similarly, for 2D charts, a point $(x_1, x_2)$ is distributed to the 4 corners of the cell it falls into ($k_{u,v}, k_{u+1,v}, k_{u,v+1}, k_{u+1,v+1}$).</p>
<p>The weight is split based on the product of the linear weights in each dimension:</p>
<p>
    $$ w_{corner} = w_{total} \cdot (1-\alpha_1)(1-\alpha_2) \quad \text{(e.g., top-left)} $$
</p>
</body>
</html>
